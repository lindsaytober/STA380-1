---
title: "STA380_HW2_Yuan_Tober"
author: "Billy Yuan"
date: "August 16, 2016"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Flights at ABIA

```{r P1_setup,echo=FALSE}
library(ggplot2)
library(reshape2)
library(gridExtra)
library(grid)
library(knitr)

planes = read.csv("https://raw.githubusercontent.com/billy-yuan/Datasets/master/ABIA_airlines.csv")

planes = planes[,c(-1,-2)] # select all columns except first 2

## convert from num/int to factors
planes$DepTime = factor(planes$DepTime)
planes$Month = factor(planes$Month)
planes$DayOfWeek = factor(planes$DayOfWeek)
planes$FlightNum = factor(planes$FlightNum)
planes$Cancelled = factor(planes$Cancelled)
planes$Diverted = factor(planes$Diverted)

## Change name to "US Airways Inc." The asteriks mean that flight data from October to December
## were excluded because it merged with American Airlines in 2008.
levels(planes$Airline)[16] = "US Airways Inc. ***"

## Multi-plot function with shared legend
## https://github.com/hadley/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {
  
  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position, legend.title=element_blank()))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)
  
  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))
  grid.newpage()
  grid.draw(combined)
  
}
# End function
```

There comes a time in a person's life when he or she has to travel to a place that is too far to reach by car, and there is no choice but to resort to air travel. Unfortunately, air travel comes with a litany of issues, some of which include overpriced food, long security lines, and crying babies. Among all of the possible issues associated with air travel, a delayed flight may be the most frustrating for customers. Fortunately, by using flight data from all flights that either landed at or departed from Austin-Bergstrom International Airport (APIA), we could pick flights that have the least probability of getting delayed.

This analysis will address the following three questions that relate to choosing flights that have the least likelihood of getting delayed:

* What are the best times of the year and week to book air travel?
* Which airlines have the most delays?
* How does your destination impact the answers to the first 2 questions?

For analysis purposes, we will assume any departure delay greater than 10 minutes to be a delayed flight regardless of what percentile this number may be bucketed to. Even if 10 minutes is considered below average for a flight during the Christmas season, a delay of 10 minutes is still infuriating for the customer. 

### Best Times to Travel (on Average)

To begin, let's understand the general trends of delayed flights. The chart below shows how the time of year impacts the number of delayed flights.

```{r delay_by_month_chart, echo=FALSE,fig.align='center',warning=FALSE}
planes$Delay = ifelse(planes$DepDelay > 10, "Delayed","No Delay") # create dummy column for delays

## bar chart of volume + number of delays. NA's omitted for visual purposes
ggplot(na.omit(planes[planes$Origin == 'AUS',c('Month','Delay')]), aes(Month, fill=Delay)) + 
  geom_bar() + ggtitle('Number of Flights Leaving Austin by Month') + 
  labs(y='Number of Flights') +
  scale_fill_discrete(name="Delay?")
```
```{r,echo=FALSE,warning=FALSE}
## ADD TABLE OF % DELAY BY MONTH
month_delay = na.omit(planes[planes$Origin == 'AUS',c('Month','Delay')])
delay_matrix=as.matrix(100*table(month_delay)[,1]/(table(month_delay)[,1]+table(month_delay)[,2]))
colnames(delay_matrix) = "% Flights Delayed during Month"
rownames(delay_matrix) = c(1:12)
kable(delay_matrix,row.names=c(1:12),digits=1)
```

```{r dealy_by_weekday_chart, echo=FALSE,fig.align='center'}
ggplot(na.omit(planes[planes$Origin=='AUS',c('DayOfWeek','Delay')]), aes(DayOfWeek, fill=Delay)) + 
  geom_bar() + ggtitle('Number of Flights by Day of Week') + 
  labs(x='Day of Week (1 = Monday)',y='Number of Flights Leaving Austin by Day of Week') +
  scale_fill_discrete(name="Delay?")
```
```{r, echo=FALSE,warning=FALSE}
day_delay = na.omit(planes[planes$Origin=='AUS',c('DayOfWeek','Delay')])
day_matrix=as.matrix(100*table(day_delay)[,1]/(table(day_delay)[,1]+table(day_delay)[,2]))
colnames(day_matrix) = "% Flights Delayed by Day"
rownames(day_matrix) = c(1:7)
kable(day_matrix,row.names=c(1:7),digits=1)

## ADD TABLE OF % DELAYS BY DAY OF WEEK
```

According to both of these charts, the month and day with the lowest percentage of delayed flights is October and Saturday, respectively. However, these percentages aggregate all destinations and airlines together.

### Airlines to Avoid

Next, let's look at which airlines have the highest percentage of delayed flights. 

```{r delay_by_airline_chart,echo=FALSE,fig.align='center'}

## tables to create % delayed flight charts by airline
depdelay_num = table(planes$Airline[planes$DepDelay >= 10])
total_num = table(planes$Airline)



worst_dep_delay= sort((depdelay_num/total_num),decreasing=FALSE)

ggplot(as.data.frame(worst_dep_delay), mapping = aes(x=Var1,y=100*Freq)) + coord_flip() +
  geom_bar(stat="identity") + ggtitle("% Flights with >= 10 min Departure Delay by Airline") + 
  labs(x='Airline',y='% of Flights with >=10 min Departure Delay')
```

<font size="0.5">***US Airways data only available from January-September 2008. October-December data is combined with American Airlines due to a merger.</font>

The two worst airlines for delays are Atlantic Southeast Airlines and Comair. In addition to those two, the percentage of flights with departure delays in 2008 from seven different airlines exceeded 20%. 

However, airlines don't have flights to every city and may specialize in certain regions. For example, in 2008, Southwest Airlines did not have any flights to New York City from Austin. If there are multiple airlines that fly to a city, how can a customer know which airline to take? And if that city has multiple airports, which airport should he or she choose?

### Tale of Cities with Two Airports: Choosing the Best Airlines and Months to Travel

While customers usually have limited flexibility on travel dates, times, and destinations, they often have most control over flight options when traveling to a city with multiple airports (e.g., Dallas/Ft. Worth metroplex is serviced by both DFW and Dallas Love Field).

Suppose customer 'Mary' has extended holiday time and is planning to visit three cities, each of which has two major airports (New York City, Dallas, and Chicago).  Mary will be flying out of Austin and is looking to minimize the chance of getting a delay for each of her trips. To support her decision on when and how to fly to each city, she has three questions:

* For what months should Mary book her flights?
* For each city, which airport should she fly to ?
* Which airlines should she choose?

Charts of % departure delays, by month and airline, for each city she plans to visit are shown below.

<center><h5>% Flights to NYC with >10 min Departure Delay by Airline</h5></center>

```{r NYC, echo=FALSE,warning=FALSE}

## Make a new dataframe with only flights leaving from Austin
planes.dest = planes[planes$Dest != 'AUS',]

## Concatenate airline and airport for the airline plots below
planes.dest$airline_dest = paste(planes.dest$Airline, planes.dest$Dest)

## Mask that will show only 
NYC_f = planes.dest$City == 'NYC'

## Add new column for "City" so that cities with multiple airlines can be aggregated
planes.dest$City[planes.dest$Dest == 'JFK' | planes.dest$Dest == 'EWR' | planes.dest$Dest == 'LGA'] = 'NYC'

## Copy and paste from my code...
NYC_f = planes.dest$City == 'NYC'
DAL_f = planes.dest$City == 'Dallas'
CHI_f = planes.dest$City == 'Chicago'

NYC.flights = (planes.dest[NYC_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])


JFK.flights = NYC.flights[NYC.flights$Dest == 'JFK',] # Look at JFK
EWR.flights = NYC.flights[NYC.flights$Dest == 'EWR',]

JFK.delays = table(JFK.flights$Month[JFK.flights$DepDelay >= 10]) / table(JFK.flights[,c('Month')]) # 15 minute dep delay is 75%
EWR.delays = table(EWR.flights$Month[EWR.flights$DepDelay >= 10]) / table(EWR.flights[,c('Month')])

NYC.delays = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10],
                   NYC.flights$Month[NYC.flights$DepDelay>=10])
NYC.total = table(NYC.flights$airline_dest,NYC.flights$Month)
## End

## Make a new dataframe for only flights going to NYC. This will be used for all the charts.

NYC.flights = (planes.dest[NYC_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])

## Charts for NYC start here

# January
a1.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '1'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '1'])

a1.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '1'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '1'])

a1 = a1.1/a1.2

# February
a2.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '2'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '2'])

a2.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '2'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '2'])

a2 = a2.1/a2.2

# March
a3.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '3'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '3'])

a3.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '3'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '3'])

a3 = a3.1/a3.2

# April
a4.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '4'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '4'])

a4.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '4'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '4'])

a4 = a4.1/a4.2

# May
a5.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '5'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '5'])

a5.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '5'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '5'])

a5 = a5.1/a5.2

# June
a6.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '6'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '6'])

a6.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '6'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '6'])

a6 = a6.1/a6.2

# October
a7.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '7'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '7'])

a7.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '7'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '7'])

a7 = a7.1/a7.2

# August
a8.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '8'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '8'])

a8.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '8'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '8'])

a8 = a8.1/a8.2

# September
a9.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '9'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '9'])

a9.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '9'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '9'])

a9 = a9.1/a9.2



# October
a10.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '10'],
      NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '10'])

a10.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '10'],
      NYC.flights$DayOfWeek[NYC.flights$Month == '10'])

a10 = a10.1/a10.2 # Best day of week to fly

# November
a11.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '11'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '11'])

a11.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '11'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '11'])

a11 = a11.1/a11.2 # Best day of week to fly

# December
a12.1 = table(NYC.flights$airline_dest[NYC.flights$DepDelay >=10 & NYC.flights$Month == '12'],
              NYC.flights$DayOfWeek[NYC.flights$DepDelay>=10 & NYC.flights$Month == '12'])

a12.2 = table(NYC.flights$airline_dest[NYC.flights$Month == '12'],
              NYC.flights$DayOfWeek[NYC.flights$Month == '12'])

a12 = a12.1/a12.2 # Best day of week to fly

## NYC Plots - % Flights with Departure Delay >= 10 Min. Airline/Day of week by month

nyc1 = ggplot(melt(t(a1)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('January')

nyc2 = ggplot(melt(t(a2)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('February')

nyc3 = ggplot(melt(t(a3)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay')+ 
  ggtitle('March')

nyc4 = ggplot(melt(t(a4)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + 
  ggtitle('April')

nyc5 = ggplot(melt(t(a5)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + ggtitle('May')

nyc6 = ggplot(melt(t(a6)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('June')

nyc7 = ggplot(melt(t(a7)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('July')

nyc8 = ggplot(melt(t(a8)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('August')

nyc9 = ggplot(melt(t(a9)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('September')

nyc10 = ggplot(melt(t(a10)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('October')

nyc11 = ggplot(melt(t(a11)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('November')

nyc12 = ggplot(melt(t(a12)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('December')
```

```{r NYC plot,echo=FALSE, fig.width=9,fig.height=11,fig.align='center',warning=FALSE}
# Multi-plot
grid_arrange_shared_legend(nyc1,nyc2,nyc3,nyc4,
                           nyc5,nyc6,nyc7,nyc8,
                           nyc9,nyc10,nyc11,nyc12,
                           ncol=3,nrow=4)
```

<center><h5>% Flights to DAL with >10 min Departure Delay by Airline</h5></center>

```{r Dallas,echo=FALSE,warning=FALSE}

## Add new column for "City" so that cities with multiple airlines can be aggregated
planes.dest$City[planes.dest$Dest == 'DAL' | planes.dest$Dest == 'DFW'] = 'Dallas'

## Copy and paste from my code...
NYC_f = planes.dest$City == 'NYC'
DAL_f = planes.dest$City == 'Dallas' & planes.dest$Airline != 'Comair Inc.'
CHI_f = planes.dest$City == 'Chicago'

DAL.flights = (planes.dest[DAL_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])

DAL.flights = DAL.flights[DAL.flights$Dest == 'DAL',] # Look at DAL
DFW.flights = DAL.flights[DAL.flights$Dest == 'DFW',]

DAL.delays = table(DAL.flights$Month[DAL.flights$DepDelay >= 10]) / table(DAL.flights[,c('Month')]) # 15 minute dep delay is 75%
DFW.delays = table(DFW.flights$Month[DFW.flights$DepDelay >= 10]) / table(DFW.flights[,c('Month')])

DAL.delays = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10],
                   DAL.flights$Month[DAL.flights$DepDelay>=10])
DAL.total = table(DAL.flights$airline_dest,DAL.flights$Month)
## End

## Make a new dataframe for only flights going to DAL. This will be used for all the charts.

DAL.flights = (planes.dest[DAL_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])

## Charts for DAL start here

# January
d1.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '1'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '1'])

d1.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '1'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '1'])

d1 = d1.1/d1.2


# February
d2.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '2'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '2'])

d2.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '2'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '2'])

d2 = d2.1/d2.2

# March
d3.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '3'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '3'])

d3.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '3'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '3'])

d3 = d3.1/d3.2

# April
d4.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '4'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '4'])

d4.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '4'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '4'])

d4 = d4.1/d4.2

# May
d5.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '5'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '5'])

d5.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '5'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '5'])

d5 = d5.1/d5.2

# June
d6.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '6'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '6'])

d6.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '6'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '6'])

d6 = d6.1/d6.2

# October
d7.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '7' &
                                        DAL.flights$Airline != 'American Eagle Airlines Inc.'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '7'])

d7.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '7' & DAL.flights$Airline != 'American Eagle Airlines Inc.'], DAL.flights$DayOfWeek[DAL.flights$Month == '7' & DAL.flights$Airline != 'American Eagle Airlines Inc.' ])


d7 = d7.1/d7.2

# August
d8.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '8'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '8'])

d8.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '8'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '8'])

d8 = d8.1/d8.2

# September
d9.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '9'],
             DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '9'])

d9.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '9'],
             DAL.flights$DayOfWeek[DAL.flights$Month == '9'])

d9 = d9.1/d9.2



# October
d10.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '10'],
              DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '10'])

d10.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '10'],
              DAL.flights$DayOfWeek[DAL.flights$Month == '10'])

d10 = d10.1/d10.2 # Best day of week to fly

# November
d11.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '11'],
              DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '11'])

d11.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '11'],
              DAL.flights$DayOfWeek[DAL.flights$Month == '11'])

d11 = d11.1/d11.2 # Best day of week to fly

# December
d12.1 = table(DAL.flights$airline_dest[DAL.flights$DepDelay >=10 & DAL.flights$Month == '12'],
              DAL.flights$DayOfWeek[DAL.flights$DepDelay>=10 & DAL.flights$Month == '12'])

d12.2 = table(DAL.flights$airline_dest[DAL.flights$Month == '12'],
              DAL.flights$DayOfWeek[DAL.flights$Month == '12'])

d12 = d12.1/d12.2 # Best day of week to fly

## DAL Plots - % Flights with Departure Delay >= 10 Min. Airline/Day of week by month

DAL1 = ggplot(melt(t(d1)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('January')

DAL2 = ggplot(melt(t(d2)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('February')

DAL3 = ggplot(melt(t(d3)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay')+ 
  ggtitle('March')

DAL4 = ggplot(melt(t(d4)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + 
  ggtitle('April')

DAL5 = ggplot(melt(t(d5)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + ggtitle('May')

DAL6 = ggplot(melt(t(d6)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('June')

DAL7 = ggplot(melt(t(d7)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('July')

DAL8 = ggplot(melt(t(d8)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('August')

DAL9 = ggplot(melt(t(d9)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('September')

DAL10 = ggplot(melt(t(d10)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('October')

DAL11 = ggplot(melt(t(d11)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('November')

DAL12 = ggplot(melt(t(d12)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('December')
```

```{r Dallas plot,echo=FALSE, fig.width=9,fig.height=11, fig.align='center',warning=FALSE}
# Multi-plot
grid_arrange_shared_legend(DAL1,DAL2,DAL3,DAL4,
                           DAL5,DAL6,DAL7,DAL8,
                           DAL9,DAL10,DAL11,DAL12,
                           ncol=3,nrow=4)
```

<center><h5>% Flights to Chicago with >10 min Departure Delay by Airline</h5></center>

```{r CHI, echo=FALSE,warning=FALSE}

## Add new column for "City" so that cities with multiple airlines can be aggregated
planes.dest$City[planes.dest$Dest == 'ORD' | planes.dest$Dest == 'MDW'] = 'Chicago'

## Copy and paste from my code...
NYC_f = planes.dest$City == 'NYC'
DAL_f = planes.dest$City == 'Dallas'
CHI_f = planes.dest$City == 'Chicago' & planes.dest$Airline != 'Mesa Airlines Inc.'

CHI.flights = (planes.dest[CHI_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])


ORD.flights = CHI.flights[CHI.flights$Dest == 'ORD',] # Look at ORD
MDW.flights = CHI.flights[CHI.flights$Dest == 'MDW',]

ORD.delays = table(ORD.flights$Month[ORD.flights$DepDelay >= 10]) / table(ORD.flights[,c('Month')]) # 15 minute dep delay is 75%
MDW.delays = table(MDW.flights$Month[MDW.flights$DepDelay >= 10]) / table(MDW.flights[,c('Month')])

CHI.delays = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10],
                   CHI.flights$Month[CHI.flights$DepDelay>=10])
CHI.total = table(CHI.flights$airline_dest,CHI.flights$Month)
## End

## Make a new dataframe for only flights going to CHI. This will be used for all the charts.

CHI.flights = (planes.dest[CHI_f,][,c('Month','DayOfWeek','Airline', 'Dest', 'DepDelay','airline_dest')])

## Charts for NYC start here

# January
c1.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '1'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '1'])

c1.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '1'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '1'])

c1 = c1.1/c1.2

# February
c2.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '2'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '2'])

c2.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '2'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '2'])

c2 = c2.1/c2.2

# March
c3.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '3'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '3'])

c3.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '3' & 
                                        CHI.flights$Airline != 'Mesa Airlines Inc.'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '3' &
                                   CHI.flights$Airline != 'Mesa Airlines Inc.'])

c3 = c3.1/c3.2

# April
c4.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '4'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '4'])

c4.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '4'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '4'])

c4 = c4.1/c4.2

# May
c5.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '5'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '5'])

c5.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '5'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '5'])

c5 = c5.1/c5.2

# June
c6.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '6'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '6'])

c6.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '6'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '6'])

c6 = c6.1/c6.2

# October
c7.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '7'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '7'])

c7.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '7'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '7'])

c7 = c7.1/c7.2

# August
c8.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '8'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '8'])

c8.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '8'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '8'])

c8 = c8.1/c8.2

# September
c9.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '9'],
             CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '9'])

c9.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '9'],
             CHI.flights$DayOfWeek[CHI.flights$Month == '9'])

c9 = c9.1/c9.2



# October
c10.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '10'],
              CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '10'])

c10.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '10'],
              CHI.flights$DayOfWeek[CHI.flights$Month == '10'])

c10 = c10.1/c10.2 # Best day of week to fly

# November
c11.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '11'],
              CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '11'])

c11.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '11'],
              CHI.flights$DayOfWeek[CHI.flights$Month == '11'])

c11 = c11.1/c11.2 # Best day of week to fly

# December
c12.1 = table(CHI.flights$airline_dest[CHI.flights$DepDelay >=10 & CHI.flights$Month == '12'],
              CHI.flights$DayOfWeek[CHI.flights$DepDelay>=10 & CHI.flights$Month == '12'])

c12.2 = table(CHI.flights$airline_dest[CHI.flights$Month == '12'],
              CHI.flights$DayOfWeek[CHI.flights$Month == '12'])

c12 = c12.1/c12.2 # Best day of week to fly

## CHI Plots - % Flights with Departure Delay >= 10 Min. Airline/Day of week by month

chi1 = ggplot(melt(t(c1)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('January')

chi2 = ggplot(melt(t(c2)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('February')

chi3 = ggplot(melt(t(c3)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay')+ 
  ggtitle('March')

chi4 = ggplot(melt(t(c4)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + 
  ggtitle('April')

chi5 = ggplot(melt(t(c5)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') + ggtitle('May')

chi6 = ggplot(melt(t(c6)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('June')

chi7 = ggplot(melt(t(c7)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('July')

chi8 = ggplot(melt(t(c8)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('August')

chi9 = ggplot(melt(t(c9)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('September')

chi10 = ggplot(melt(t(c10)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('October')

chi11 = ggplot(melt(t(c11)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('November')

chi12 = ggplot(melt(t(c12)), mapping=aes(x=Var1,y=value,colour=Var2)) + 
  geom_line() + coord_cartesian(ylim=c(0,1)) + 
  scale_x_continuous(breaks=seq(1,7,1)) +
  labs(x='Day of the Week (1 = Monday)', y='% Flights, Dep Delay') +
  ggtitle('December')
```

```{r CHI plot,echo=FALSE, fig.width=9,fig.height=11,fig.align='center',warning=FALSE}
# Multi-plot
grid_arrange_shared_legend(chi1,chi2,chi3,chi4,
                           chi5,chi6,chi7,chi8,
                           chi9,chi10,chi11,chi12,
                           ncol=3,nrow=4)
```

According to these 3 groups of charts, September and October appear to be the best times to fly, which is consistent with what the very first chart of this analysis showed. However, the best airlines for each city vary. Below is a sample itinerary that Mary could use:

```{r sample_itinerary, echo=FALSE,fig.align='center'}

nyc_it = c("JetBlue Airways JFK","October","Wednesday")
dal_it = c("American Airlines DFW", "November", "Tuesday")
chi_it = c("Southwest Airlines MDW", "September", "Wednesday")

itinerary = rbind(nyc_it,dal_it,chi_it)
rownames(itinerary) = c("New York City", "Dallas", "Chicago")
colnames(itinerary) = c("Airline and Airport", "Month", "Day of Week")
kable(itinerary, caption='Sample Itinerary')
```

Segmenting data to match the scope of the problem is extremely important. In this case, the problem is figuring out which airlines to take to cities that have 2 airports. For example, although Southwest Airlines had the second-highest percentage of delayed flights when including all of its destinations, its flights to Midway Airport in Chicago are relatively punctual between June and September. However, because this analysis only contains data fron 2008, one-off events such as storms or emergencies may skew the data. Regardless of this limitation, the general trends seem to be consistent: flights during the holidays experience the most delays, and flights during the middle of the week have the least delays. Beyond this, generalizations are slightly harder to make.



# Author Attribution

Author attribution is one of the many applications of text analysis. Given a group of documents by various authors, how accurately can we predict the authorship of an out-of-sample document? Reuters, a well-known news source, is a prime subject for conducting author attribution analysis.

```{r P2_setup,include=FALSE}
# Setup
{
  library(tm)
  library(nnet)
  library(klaR)
  library(XML)
  library(ggplot2)
  library(knitr)
  
  # ## BILLY
  setwd("~/Dropbox/MSBA/Summer 2016/STA380-master")
  source('./R/textutils.R')
  
  ## LINDSAY
  # setwd("~/Documents/MSBA/GitHub/STA380-HW2")
  # source('textutils.R')
}

# Data import and text processing
{
  ## Read in files
  {
    # ## BILLY 
    author_dirs_train = Sys.glob('./data/ReutersC50/C50train/*') # train
    author_dirs_test = Sys.glob('./data/ReutersC50/C50test/*') # test
    
    ## LINDSAY
    # author_dirs_train = Sys.glob('dataset/ReutersC50/C50train/*') # train
    # author_dirs_test = Sys.glob('dataset/ReutersC50/C50test/*') # test
    
    # readerPlain function, used to apply file names
    readerPlain = function(fname){
      readPlain(elem=list(content=readLines(fname)), 
                id=fname, language='en') }
  }
  
  ## Import and process training set
  {
    file_list_train = NULL
    labels_train = NULL
    
    for(author in author_dirs_train) {
      author_name = substring(author, first=29)
      files_to_add = Sys.glob(paste0(author, '/*.txt'))
      file_list_train = append(file_list_train, files_to_add)
      labels_train = append(labels, rep(author_name, length(files_to_add)))
      }
    
    all_docs_train = lapply(file_list_train, readerPlain) 
    names(all_docs_train) = file_list_train
    names(all_docs_train) = sub('.txt', '', names(all_docs_train))
    
    my_corpus_train = Corpus(VectorSource(all_docs_train))
    names(my_corpus_train) = file_list_train
  
    # Preprocessing
    my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
    my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) # remove excess white-space
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART")) # remove stopwords
    
    # Document Term Matrix
    DTM_train = DocumentTermMatrix(my_corpus_train)
    X_train = as.matrix(DTM_train)
  }
  
  ## Import and process test set
  {
    file_list_test = NULL
    labels_test = NULL
    
    for(author in author_dirs_test) {
      author_name = substring(author, first=28)
      files_to_add = Sys.glob(paste0(author, '/*.txt'))
      file_list_test = append(file_list_test, files_to_add)
      labels_test = append(labels, rep(author_name, length(files_to_add)))
      }
    
    all_docs_test = lapply(file_list_test, readerPlain) 
    names(all_docs_test) = file_list_test
    names(all_docs_test) = sub('.txt', '', names(all_docs_test))
    
    my_corpus_test = Corpus(VectorSource(all_docs_test))
    names(my_corpus_test) = file_list_test
    
    my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
    my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) # remove excess white-space
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART")) # remove stopwords
    
    DTM_test = DocumentTermMatrix(my_corpus_test)
    
    X_test = as.matrix(DTM_test)
  }
}

# Prepare for modeling
{
  ## Create training set for PCA logistic regression 
  {
    train_log = as.data.frame(X_train)
    train_log$target = 0
    train_log$new_word = 0
    
    test_log = as.data.frame(X_test)
    test_log$target = 0
  }
  
  ## Create target variables that correspond to each author
  {
    for (i in 1:50) {
      start_idx = 50*(i-1) + 1
      end_idx = (50*i)
      
      train_log$target[start_idx:end_idx] = i
      test_log$target[start_idx:end_idx] = i
    }
    
    train_log$target = factor(train_log$target)
    test_log$target = factor(test_log$target)
  }
  
  ## Create X and Y training sets
  {
    X.log = as.matrix(train_log[,names(train_log)!='target'])
    y.log = train_log$target
  }
  
  ## Create X and Y test sets
  {
    X.test = as.matrix(test_log[,names(test_log)!='target'])
    y.test = test_log$target
  
    test.matrix = matrix(data=0,nrow=dim(X.log)[1],ncol=dim(X.log)[2]) # test matrix with same columns as train
    colnames(test.matrix) = colnames(X.log)
  }
  
  ## Build function to sapply over test matrix
  {
    match_columns = which(colnames(X.test) %in% colnames(test.matrix))
    match_train = which(colnames(test.matrix) %in% colnames(X.test[,match_columns]))
  
    test.matrix[,match_train] = X.test[,match_columns]
  }
}

```

### Preparing the Data
The Reuters C50 corpus contains articles written by 50 different authors, where each author has 50 'training' articles and 50 'testing' articles.  The training articles were used to build a Document Term Matrix for predicting author attribution of the 50 testing articles.  This required some transformation of the data in pre-processing (e.g., changing case to all lowercase; removing numbers, punctuation, and excess white-space; and eliminating 'stop words' that are common enough in the English language to remove valuable insight for author attribution) to allow for effective text analysis.

Given the training and testing data sets are two separate corpuses, aligning the terms used in their Document Term Matrices to ensure matching dimensions is critical to accurately modeling author attribution. Establishing a single 'umbrella' term list for the two corpuses requred a couple of assumptions.  First, new terms existing in articles from the testing data set (i.e., terms present in the testing articles that are not present in the training articles) would not have any weight indicators to assist in attributing the articles to a given author, so these 'new' terms were excluded from the umbrella term list. Second, terms present in the training articles that were not present in the testing articles were kept in the umbrella term list as they were used in building the model on the training data (e.g., for Naive Bayes, the weight matrix is constructed using the bag of words model, which required usage of all words to accurately build weights).  This resulted in approximately 10,000 irrelevant words for the test data set, but it ensured that the weights of the common words were accurate for a given document and author.

### Model Summary
Two models were selected to predict author attribution of the testing articles: (1) Principal Component Multinomial Logistic Regression and (2) Naive Bayes.

#### Model 1: Principal Component Multinomial Logistic Regression
```{r PCR_model,include=FALSE}

# Run PCR (Principal Component Multinomial Logistic Regression)
{
## Create S-Matrix
{
  pc_author = prcomp(X.log, center=TRUE)
  K = 100
  V = pc_author$rotation[,1:K]
  S = X.log %*% V # S-Matrix
}
  
## Run Multinomial Logistic Regression
{
  # Model
  model_multi = multinom(y.log ~ .,data=as.data.frame(S),MaxNWts=10200)
  
  # Predictions
  predict_log = predict(model_multi,as.matrix(test.matrix)%*%V)
}
  
## Get PCA Results
{
  # Get accuracy
  get_accuracy = function(k){
    trues=0
    for (i in 1:k){
      if (predict_log[i] == y.test[i]) {
        trues=trues+1
      } else {
        trues=trues
      }
    }
    trues/k
  }
  get_accuracy(2500)
  
  # Create testing matrix to get author accuracy
  pcr_results_df = matrix(0,nrow=2500,ncol=2)
  colnames(pcr_results_df) = c("Actual","Predict")
  pcr_results_df[,2] = predict_log
  
  # Fill the results matrix with with actual authors
  for (author in 1:50) {
    start_idx = 50*(author-1) + 1
    end_idx = (50*author)
    
    pcr_results_df[start_idx:end_idx,1] = author
  }
  
  # Test Accuracy Results
  cat("PCR Test Accuracy is",100*sum(pcr_results_df[,1] == pcr_results_df[,2])/dim(pcr_results_df)[1],"%")
  PCR_test_accuracy = sprintf("%.2f %%",100*sum(pcr_results_df[,1] == pcr_results_df[,2])/dim(pcr_results_df)[1])
  PCR_test_accuracy
  
}
}

```
Conducting Principal Compenent Analysis (PCA) on the training articles provided contexts, or loadings, to apply to the Document Term Matrix.  Given each of the 50 authors had 50 articles included in the training set, PCA resulted in 2,500 principal components to capture variance across the 2,500 documents within the training corpus.  With limitations on computational power, the first 100 principal components were selected for running multinomial logistic regression.  This subset of 100 principal components accounted for 45% of the variance and still took a significant time to process.

A multinomial logistic regression model was trained to predict authors on the 100 principal components for all training articles, then applied to the testing articles to predict author identities in the out-of-sample data set. Multinomial logistic regression using PCA resulted in a test accuracy of 65.46%.

#### Model 2: Naive Bayes
```{r NB_model,include=FALSE}
# Run Naive Bayes
{
## Create authors reference
{
  # Create authors list
  authors = NULL
  
  for (item in author_dirs_train) {
    authors = append(authors, substring(item, first=29))
  }
  
  # Create index for authors for Naive Bayes
  authors_index = as.data.frame(authors)
  train_index = c(1:50)
}

## Build 'master' training set for Naive Bayes
{
  # Smoothing factor
  smooth_factor = 1/nrow(X_train)
  
  # Training table  
  X_NB = matrix(data=NA, nrow=50, ncol=dim(X_train)[2])
  
  for (i in 1:50) {
    start_idx = 50*(i-1) + 1
    end_idx = (50*i)
    
    author_train = X_train[start_idx:end_idx,]
    
    X_NB[i,] = colSums(author_train + smooth_factor)
    X_NB[i,] = X_NB[i,]/sum(X_NB[i,])
  }
}

## Build testing matrix
{
  # Create results matrix for testing
  nb_results_df = matrix(0,nrow=2500,ncol=2)
  colnames(nb_results_df) = c("Actual","Predict")
  
  # Fill the results matrix with with actual authors
  for (author in 1:50) {
    start_idx = 50*(author-1) + 1
    end_idx = (50*author)
    
    nb_results_df[start_idx:end_idx,1] = author
  }
}

## Run loop to make predictions
{
  for (doc in 1:2500) {
  results_store = c()
  
  for (author in 1:50) {
    results_store[author] = sum(test.matrix[doc,]*log(X_NB[author,]))
  }
  
  max_prob = max(results_store)
  nb_results_df[doc,2] = which(results_store == max_prob)
  }
}

## Get Naive Bayes results
{
  # Test Accuracy
  cat("Naive Bayes Test Accuracy is",100*sum(nb_results_df[,1] == nb_results_df[,2])/dim(nb_results_df)[1],"%")
  NB_test_accuracy = sprintf("%.2f %%",100*sum(nb_results_df[,1] == nb_results_df[,2])/dim(nb_results_df)[1])
  NB_test_accuracy
}
}

```
For Naive Bayes, the Document Term Matrix was used to create a training table of multinomial probability vectors by author. Each multinomial probability vector was calculated by aggregating the scores by term across documents written by a given author, adding a smoothing factor (1/2500) to ensure non-zero totals, and taking the respective probability for that term against the author's full volume of terms.  The resulting multinomial probability vectors provide a 'bag of words' situational probability of selecting that word at random from within the host of articles written by that author.

Using the training table of multinomial probability vectors by author, log probabilities for each article within the testing data set were calculated under the Naive Bayes model.  Whichever author resulted in the highest sum of log probabilities was taken as the 'predicted' author. The Naive Bayes model resulted in a test accuracy of 55.28%.


### Model Evaluation
Taken at face value, the Naive Bayes model outperformed the Principal Component Multinomial Logistic Regression (PCR) model by almost 10% (64.56% vs. 55.28%, respectively).  However, model accuracy varied by author:

```{r model_accuracy_comparison_table,fig.align='center',echo=FALSE}

# Where do the models make the most errors?
{
  ## Set up author accuracy table
  {
    author_accuracy_table = matrix(0,nrow=50,ncol=3)
    colnames(author_accuracy_table) = c('Author','NB Accuracy','PCR Accuracy')
    author_accuracy_table[,1] = as.vector(authors)
  }
  
  ## Author accuracy - Naive Bayes
  for (author_idx in 1:50){
    author_results_store = nb_results_df[nb_results_df[,2] == author_idx,] 
    author_accuracy_table[author_idx,2] = sum(author_results_store[,1] == author_results_store[,2])/dim(author_results_store)[1]
  }
  
  ## Author accuracy - PCA
  for (author_idx in 1:50){
    author_results_store = nb_results_df[pcr_results_df[,2] == author_idx,] 
    author_accuracy_table[author_idx,3] = sum(author_results_store[,1] == author_results_store[,2])/dim(author_results_store)[1]
  }
  
  ## Complete Author Accuracy Table
  {
    author_accuracy_table[order(author_accuracy_table[,2],decreasing=FALSE),] # Order by NB Accuracy
    author_accuracy_table[order(author_accuracy_table[,3],decreasing=FALSE),] # Order by PCR Accuracy
    format_percent <- function(x) sprintf("%.2f %%",100*as.numeric(x))
    author_accuracy_table_formatted = data.frame(author_accuracy_table[,1], apply(author_accuracy_table[,2:3],2,format_percent))
    # Fix column names
    colnames(author_accuracy_table_formatted) = c('Author','NB Accuracy','PCR Accuracy')
  }
  
  ## Comparison
  model_diff = as.numeric(author_accuracy_table[,2]) - as.numeric(author_accuracy_table[,3])
  NB_beat_PCR = author_accuracy_table[which(model_diff == max(model_diff)),] # NB beats PCR by the most amount
  PCR_beat_NB = author_accuracy_table[which(model_diff == min(model_diff)),] # PCR beats NB by the most amount
}

# Print Table
kable(author_accuracy_table_formatted, caption='Author Prediction Accuracy by Model')
  
```

As the chart shows, Naive Bayes showed the most benefit over PCR for author Martin Wolk, with a test accuracy of 91.18% for NB and 65.39% for PCR. On the flip side, PCR performed best over Naive Bayes for author Sarah Davison, with a test accuracy of 69.70% for PCR and 44.44% for NB. Aside from the biggest gaps in accuracy between the models, the chart comparing model accuracy reveals that some authors  had high success on both models (e.g., Aaron Pressman, Fumiko Fujisaki) or low success on both models (e.g., David Lawder, Jane Macartney). What might be driving these issues?

We can start to assess this question by looking at the plot of authors against the top two Principal Components:
```{r PCR_plot,echo=FALSE,fig.align='center'}
# Create plot of all authors for first 2 loadings, colored by accuracy / difference 
{
  ## Set up data
  {
    author_plot_pca = prcomp((X_NB),center=TRUE)
    dim(author_plot_pca$rotation)
    author_plot_pca_data = as.data.frame(author_plot_pca$x[,1:2])
    author_plot_pca_data['38-46'] = 1
    author_plot_pca_data['38-46'][38,] = 0
    author_plot_pca_data['38-46'][46,] = 0
  }

  ## Create Plot
  {
  pcr_plot = ggplot(author_plot_pca_data,aes(x=author_plot_pca_data$PC1,y=author_plot_pca_data$PC2,label=rownames(author_plot_pca_data))) +
    ggtitle("PC1 & PC2 by Author") +
    labs(x="PC 1", y="PC 2") +
    geom_text(aes(colour=factor(author_plot_pca_data$`38-46`)),size=6) +
    scale_colour_discrete(guide=FALSE)
  }
}

# Show plot 
pcr_plot 
```

```{r 46_38_comparison,include=FALSE}

# 46 incorrectly attributed to 38  
{
  ## number of errors - PCA
  {
    PCA_total_error_46 = sum(pcr_results_df[pcr_results_df[,1] == 46,][,1] != pcr_results_df[pcr_results_df[,1] == 46,][,2])
  
    PCA_error_38_as_46 = dim(pcr_results_df[pcr_results_df[,1] == 46 & pcr_results_df[,2] == 38,])[1]
  
    cat("PCA predicted author 38 (Peter Humphrey) when the correct author was 46 (Tan EeLyn)",sprintf("%.2f %%", 100*(PCA_error_38_as_46/PCA_total_error_46)),"of the time.")
    PCA_38as46_error_percent = sprintf("%.2f %%", 100*(PCA_error_38_as_46/PCA_total_error_46))
    PCA_38as46_error_percent
  }
    
  ## number of errors - NB
  {
  names(X_NB) = colnames(test.matrix) # name the columns of X_NB to the words in corpus
  nb_46 = nb_results_df[nb_results_df[,1] == 46,] 
  table(nb_46[nb_46[,1] != nb_46[,2],][,2]) # count of mistakes by author guess
  
  sort(test.matrix[2261,],decreasing=TRUE)[1:10] # top 10 words used in article 2261, which is
  # an article in which the correct author was 46 and the model guessed 38
  
  top_10_author_46=(sort(X_NB[46,],decreasing=TRUE)[1:10]) # indexes of  10 words used by author 46
  top_10_author_38=(sort(X_NB[38,],decreasing=TRUE)[1:10]) # indexes of top 10 words used by author 38


    # top 10 words used by authors 38 and 46
  top_probs_46 = names(X_NB)[which(X_NB[46,] %in% top_10_author_46)] # top 10 words used by author 46
  top_probs_38 = names(X_NB)[which(X_NB[38,] %in% top_10_author_38)] # top 10 words used by author 38
  }
}
  
```

Authors 38 (Peter Humphrey) and 46 (Tan EeLyn) stand out as two authors  that are closer together in the plane of P1 x P2 but further away from most of the other authors.  Going back to the summary table, we see that Tan EeLyn (author 46) had test accuracies of 39.02% for NB and 38.18% for PCR.  Given the close distance to 38, we can check the portion of errors where test articles for author 46 were incorrectly attributed to author 38.  Out of 30 incorrect attributions for author 46, 15 documents (or 50.00%) were attributed to author 38.

How does this interaction play out within the Naive Bayes model?  For Naive Bayes, 55.88% of the incorrect attributions for author 46 were to author 38.  Similar to the closer loadings of PC1 and PC2 under the PCR model, we can confirm whether authors 46 and 38 have similar weights for the Naive Bayes model.  This is done easily by comparing the top 10 terms from both of authors' multinomial probability vectors:

```{r 46_38_comparison_table,echo=FALSE,fig.align='center'}
# Multinomial probability vector comparison table
{
  top10_46_vs_38 = cbind(top_probs_46, top_probs_38)
  colnames(top10_46_vs_38) = c("Author 46 (Tan EeLyn)", "Authors 38 (Peter Humphrey)")
  kable(top10_46_vs_38, caption='Top 10 Terms of Multinomial Probability Vector')
}
```

Both lists of terms are very similar, which may explain why many of the test articles for author 46 are attributed to author 38.  Take, for example, test article #2261, which had the following term frequencies:

```{r test_article,echo=FALSE,fig.align='center'}
# Test Article 2261
{
  article_2261 = as.data.frame(sort(test.matrix[2261,],decreasing=TRUE)[1:10])
  colnames(article_2261) = c('Frequency in Document')
  kable(article_2261, caption='Top 10 Terms by Frequency for Test Article #2261')
}
```

Many of these terms appear in the top 10 lists for authors 46 and 38. In addition, the higher frequency of legislature in test article #2261 may have been one of the key drivers for attributing that document to author 46 instead of 38, as 'legislature' has a higher relative multinomial probability for author 46.

While both models significantly improved the chances of correctly attributing the author - up to 64.56% for NB and 55.28% for PCR from 1/50, or 2% at random - Naive Bayes did have a slightly higher test accuracy.  In addition, the results of the Naive Bayes model were easier to interpret, given the 'bag of words' probabilities by term are more intuitive than the contextual representation of Principal Components.  As such, we prefer to use the Naive Bayes model for author attribution.  Given there were only 50 articles from each author in the training data set, we believe that the Naive Bayes model could be significantly improved by including additional training articles to further differentiate multinomial probability vectors between different authors.

</br>

# Practice with Association Rule Mining
Applying association rule mining to market baskets is a helpful method to understand and predict market basket associations.  For example, pretend that 'Paul' is marketing director of a local grocery store. Paul is interested in understanding the purchasing habits of the store's customers to inform the new layout management is rolling out in the next month.  Specifically:

* Which items should be located to the front of the store?
* Which items should be grouped together (i.e., which items are customers more likely to buy together)?  

These questions can be addressed using association rule mining, which helps to predict additional items within a customer's basket (consequents) given an initial set of market basket items (antecedents).
```{r P3_setup, include=FALSE}

# Load arules
library(arules) 
library(knitr)
# arules and tm both have 'inspect()' methods, so detach tm
# detach(package:tm, unload=TRUE)

# Read in groceries text file as a transactions object
grocerytrans <- read.transactions("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", format = c("basket"), sep = ",", rm.duplicates = FALSE)

# View the output
inspect(grocerytrans)
```

Taking transactional data on grocery purchases, we can run an 'apriori' algorithm to give Paul the information he needs. To focus in on key relationships, we will set thresholds as follows:

* Support threshold > 1%, to include only antecedents that show up in over 1% of all baskets (casting a wide net)
* Confidence threshold > 10%, to include only antecedents that have the associated consequents over 10% of the time 
* Max predictor items = 3, to include only antecedents with 3 or less items, as there is not sufficient shelf space to group a higher number of items


### Subset 1: Frequent Purchases
After running the algorithm, the first thing we notice is that eight items have no antecedents:

```{r apriori_algorithm,include=FALSE}
# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# items) <= 3
grocery_rules <- apriori(grocerytrans, 
                      parameter=list(support=.01, confidence=.1, maxlen=3))
inspect(grocery_rules)
grocery_rules_df = as(grocery_rules, "data.frame")
colnames(grocery_rules_df) = c('Rules','Support','Confidence','Lift')

```

```{r apriori_initialOutput,echo=FALSE,fig.align='center'}
# View the output
top_8_blank = (grocery_rules_df[1:8,])
kable(top_8_blank, caption='Market Basket Rules with No Antecedents')
```

These eight items are purchased on a fairly regular basis, so Paul should recommend that management place these items in easy to find places (e.g., eye level for refrigerated items) and put them towards the front of the store to the extent possible.

### Subset 2: Strong Indicators
To address Paul's second question, we can focus on associations where antecedents imply consequents with a lift greater than or equal to 2, meaning that customers are at least twice as likely to buy the consequent goods with the antecedent goods in their basket, and 50% confidence, meaning that at least half of the customers with the antecedent goods have the consequent goods in their baskets:

```{r lift_2,echo=FALSE,fig.align='center'}
## Subset -- lift >=2, confidence >= 0.5
lift_conf = as((subset(grocery_rules, subset=lift >= 2 & confidence >= 0.5)),"data.frame")
colnames(lift_conf) = c('Rules','Support','Confidence','Lift')
kable(lift_conf, caption='Market Basket Rules with Lift >= 2 & Confidence >= 0.5')
```

The primary insight from this information is that a number of different items are good indicators that a customer will buy whole milk, though most of the antecedents are either dairy or produce products.  This is likely because of the high percentage of baskets that included whole milk (25%).  Regardless, these indicators may still warrant a suggestion to put the dairy products near produce.  

### Subset 3: Stronger Indicators with More Frequent Purchases
However, some of these rules do not have very high support levels.  We can view a different segment that has at least 3% support, meaning that the antecedent goods are in at least 3% of the baskets, lowering lift to greater than or equal to 1.5 and confidence to greater than or equal to 0.3:

```{r support_3,echo=FALSE,fig.align='center'}
support_lift_conf = as((subset(grocery_rules, subset=support >= .03 & confidence >= 0.3 & lift >= 1.5)), "data.frame")

colnames(support_lift_conf) = c('Rules','Support','Confidence','Lift')
kable(support_lift_conf, caption='Market Basket Rules with Support >= 0.03, Confidence >= 0.3, & Lift >= 1.5')
```

Whole milk is still a key item, further supporting the insights from the first subset of rules.  In addition, some new rules have surfaced given the revised parameters, prompting the following potential recommendations:

* Put rolls/buns near saugages in refrigerated section to increase sales of these breakfast combos (related to rule #278)
* Put tropical fruit in a prime location to encourage purchasing of additional vegetables (related to rule # 306)
* Put root vegetables in a prime location to encourage purchasing of additional vegetables (related to rule # 316)


### Subset 4: Highest Lift
One final subset might be to look at relationships with the largest lift values, indicating antecedents that are most likely to increase the odds of a consequent being in the same basket:

```{r high_lift,echo=FALSE,fig.align='center'}
support_lift_conf = as(tail(subset(grocery_rules, subset=support >= .03 & confidence >= 0.3 & lift >= 1.5)), "data.frame")

high_lift = grocery_rules_df[order(grocery_rules_df[,4],decreasing=TRUE),][1:10,]
colnames(high_lift) = c('Rules','Support','Confidence','Lift')
kable(high_lift, caption='Market Basket Rules with 10 Highest Lift Values')
```

This segment of rules with higher lift totals suggest a couple of additional ideas for store layout:

* Ensure curd is near the whole milk and yogurt (related to rule #342)
* Consider positioning some sour cream near vegetables and some near yogurt (related to rule #357)
* Promote 'meat and potatoes' (beef and root vegetable) pairings (related to rule #60 and rule #59)


### Conclusions
Overall, the discovered item sets make sense given the skew from items that had strong representation across all market baskets (e.g., whole milk, other vegetables). Additional market basket data could help to improve confidence in some of the rules, if they hold true. As Paul continues to collect market basket data, he should revise his recommendations on product placement. If nothing else, this analysis supports ensuring that the inventory of whole milk is stocked regularly so that customers don't have to ask, 'Got Milk?'.  


</br>
