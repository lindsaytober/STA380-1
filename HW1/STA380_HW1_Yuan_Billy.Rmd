---
title: 'STA 380 Homework 1: Yuan, Billy'
author: "Billy Yuan"
date: "August 6, 2016"
output: md_document
  
---
```{r,include=FALSE}
set.seed(7)
library(knitr)
library(mosaic)
library(foreach)
library(fImport)
library(ggplot2)
library(flexclust)
```
## I. Probability Practice

### Part A

To start, the abbreviations that will be used in the solution are listed.

* Two categories of users: Truthful Clicker (_TC_) and Random Clicker (_RC_)
* Two outcomes: User answers "Yes" (_Y_) and User answers "No" (_N_)

We are given the following information:

* _P_(_Y_) = 0.65 and _P_(_N_) = 0.35
* _P_(_RC_) = 0.3 and _P_(_TC_) = 0.7
* _P_(_Y_ | _RC_) = 0.5

The question is asking for:

> _P_(_Y_ | _TC_) = ?

Calculating _P_(_Y_ | _TC_) involves using the _law of total probability_. Substituting the given information allows us to write the following:

> _P_(_Y_) = _P_(_Y_ | _TC_) $\times$ _P_(_TC_) + _P_(_Y_ | _RC_) $\times$ _P_(_RC_)

Next, we substitute the values into the equation.

> 0.65 = _P_(_Y_ | _TC_) $\times$ 0.7 + (0.5 $\times$ 0.3)

Finally, we solve for _P_(_Y_ | _TC_).

> 0.65 = _P_(_Y_ | _TC_) $\times$ 0.7 + 0.15
> 0.5 = _P_(_Y_ | _TC_) $\times$ 0.7
> 0.71 = _P_(_Y_ | _TC_)

The fraction of users who clicked "Yes" given that they were Truthful Clickers is 0.71.

### Part B

We are given the following information:

* _P_(_Postive_ | _Disease_) = 0.993
* _P_(_Negative_ | ~_Disease_) = 0.9999
* _P_(_Disease_) = 0.000025

Assuming that _P_(_Positive_) and _P_(_Negative_) are mutually exclusive events, we can also calculate the complement of these events:

* _P_(_Negative_ | _Disease_) = 0.007
* _P_(_Positive_ | ~_Disease_) = 0.0001
* _P_(~_Disease_) = 0.999975

The question is asking for the following:

> _P_(_Disease_ | _Positive_)

The question could also be rewritten using Baye's Rule:

> [_P_(_Disease_) $\times$ _P_(_Postive_ | _Disease_) ] / (_P_(_Positive_))

Both components of the numerator are given. To solve for _P_(_Positive_), the law of total probability can be used.

> _P_(_Positive_) = _P_(_Postive_ | _Disease_)*_P_(_Disease_) + _P_(_Postive_ | ~_Disease_)*_P_(~_Disease_)

> _P_(_Positive_) = (0.993)(.000025) + (0.0001)(.999975)

> _P_(_Positive_) = 0.0001248225

Now this can be plugged into Baye's Rule.

> _P_(_Disease_ | _Positive_) = [_P_(_Disease_) $\times$ _P_(_Postive_ | _Disease_) ] / (_P_(_Positive_))

> _P_(_Disease_ | _Positive_) = (0.000025 $\times$ 0.993) / 0.0001248225

> _P_(_Disease_ | _Positive_) = 0.1989 = 19.89%

Given that someone tested positive for the disease, the chance that he or she has the disease is nearly 20%. In other words, the test returns false positives almost 80% of the time. If a universal testing policy for this disease were to be implemented, four out of 5 people who tested positive wouldn't actually have the disease; the policy would be extremely ineffective.

## II. Exploratory Analysis: Green Buildings
```{r,echo=FALSE}
greendata = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv")
greendata=greendata[,c(-1,-2)]
greendata$green_rating_2 = ifelse(greendata$green_rating == 1,"Green","~Green")
greendata$class_a_2 = ifelse(greendata$class_a == 1,"Class A","Not Class A")
```

To determine whether to "go green" for her next real estate investment, a real estate developer had an on-staff "Excel guru" create a preliminary recommendation. His recommendation is that the company should build the "green" building because according to his analysis, green buildings command a higher rent, and the difference in rent would be extra revenue that would turn profitable after paying back the extra investment of "going green" within 9 years, assuming the leasing rate is 90%+.

While this analysis cannot provide a recommendation on whether to build the green building, it will point out three key fallacies in the guru's analysis that casts doubt on the causal relationship between "going green" and average rent. The three fallacies I will focus on are:

* _Using median price as the sole basis on deciding whether "going green"_
* _Failing to analyze confounding variables that may explain why green buildings are pricier_
* _Using the difference in green and non-green median rents to determine profit without subsetting the data_

So whether the developer builds the green building, she can make a more informed decision after reading the analysis below.

### How much does a higher median rent tell us?

The guru's first claim is that because the median market rent per square foot per year of a green building is $2.60 more than that of a non-green building, the difference in rent reflects the increase in value by going green. To determine whether this difference in median is significant, we could run a permutation test by shuffling the green status, then creating a histogram of the median rents of only green buildings. The histogram below is the result from 1000 resamples. It turns out that $26.40, the median rent of green buildings that the guru calculated, is statistically significant at the 95% level.

```{r,echo=FALSE}
# Shuffle data
green_shuffle = data.frame(shuffle(greendata$green_rating), greendata$Rent)

# Repeat many times
permtest2 = foreach(i = 1:1000, .combine='c') %do% {
  green_shuffle = data.frame(shuffle(greendata$green_rating), greendata$Rent)
  median(green_shuffle[green_shuffle$shuffle.greendata.green_rating. == 1,]$greendata.Rent)
}

# Plot
par(mfrow=c(1,1))
hist(permtest2, main="Median Rents of Green Buildings, 1000 Reshuffles",
                xlab="Median Rent")
median_orig = 26.4
abline(v=median_orig,col='blue',lwd=4)
```

However, using the median rent as the basis for revenue calculations could be misleading. After all, there's a 50% chance that the "actual" median rent that the real estate developer sets for her building is lower depending on other variables. Maybe no company in Austin would pay $27.60 per square foot per year, even the building were "green." Maybe companies in Austin care more about other building features. In other words, a median, even one that is statistically significant, by itself doesn't prove the value of going "green". The box plot below shows a large overlap between the rents of green and non-green buildings.


```{r,echo=FALSE}
bwplot(Rent~as.factor(green_rating),
        data=greendata[greendata$leasing_rate > 0.9,],
        xlab="Has Green Rating? (0 = No)",
        ylab="Rent ($ per sq. foot per year)",
        main="Not Seeing Green? Rent Prices vs. Green Rating",
        ylim=c(0,70),
        aspect=1.5)
```

The large overlap reveals variation in rent whether a building has a green rating or not. The real estate developer would want to understand a bit more about the differences between green and non-green buildings in the data set. So other variables must be analyzed for clues for how plausible the "green premium" is.

### Possible Confounding Variables: Being "best-in-class" matters

"Class" is a variable that the guru possibly overlooked and that may explain why the median rent of a green building is higher. It is an indicator of building quality relative to other buildings in a market. Is there a relationship between having a green rating and class?

```{r, echo=FALSE}
# min_rent_green_a= quantile(log((greendata[greendata$class_a == 1 &
#                        greendata$green_rating == 1,]$Rent)),c(0.25))
xyplot(log(Rent) ~ log(cluster_rent) | as.factor(green_rating), group=class_a, 
       data=greendata,
       strip=strip.custom(factor.levels=c("No Green Rating","Has Green Rating")), 
       ylim=c(.5,6),
       xlab="Avg. Rent of Cluster (log)",
       auto.key=list(space="top", columns=2,
       title="Class A? (1 = Yes, 0 = No)", cex.title=0.8,reverse.rows=TRUE),
       main=list(label="Effect of Class on Rent",
                 cex=0.8),
       res=20
       )

```

The lattice plot above shows the log of the average rent in a building's cluster on the x-axis and the log of rent on the y-axis. The average rent of a building's cluster was chosen to be the x-axis because it allows us to see the relative value of a building compared to other buildings in its "price group"; a building that has a higher rent than another within the same cluster rent must be more valuable due to factors other than cluster location.

The magenta points are buildings that are Class A. The blue points are buildings that are either Class B or C. The left plot contains buildings with no green rating. There are few Class A buildings that lie on bottom section of the plot. Although there is some overlap between the blue and magenta points, the rents of Class A buildings appear to be slightly higher than those of non-Class A buildings. Being class A appears to have a positive effect on rent, particularly in areas where the average cluster rent is low.

What's surprising is the right plot, which contains only buildings with a green rating. Nearly all of the buildings are Class A! Furthermore, for a given cluster-rent value, the rents of Class A buildings with and without green ratings are nearly identical. _This suggests that being Class A may have a larger effect on rent than having a green rating._ 

How do the median prices look after controlling for the class of a building? 


```{r, echo=FALSE}
median.table = as.data.frame(matrix(nrow=3,ncol=3))
rownames(median.table) = c("Green", "~Green", "")
colnames(median.table) = c("Class A", "~Class A","")
median.table[1,1] = median(greendata[greendata$green_rating == 1 &
                                     greendata$class_a == 1,]$Rent)
median.table[2,1] = median(greendata[greendata$green_rating == 0 &
                                     greendata$class_a == 1,]$Rent)
median.table[3,1] = median(greendata[greendata$class_a == 1,]$Rent)

median.table[1,2] = median(greendata[greendata$green_rating == 1 &
                                     greendata$class_a == 0,]$Rent)
median.table[2,2] = median(greendata[greendata$green_rating == 0 &
                                     greendata$class_a == 0,]$Rent)
median.table[3,2] = median(greendata[greendata$class_a == 0,]$Rent)

median.table[1,3] = median(greendata[greendata$green_rating == 1,]$Rent)
median.table[2,3] = median(greendata[greendata$green_rating == 0,]$Rent)
median.table[3,3] = median(greendata$Rent)

green.num.table = xtabs(~green_rating_2 + class_a_2, data=greendata)

kable(median.table,caption="Median Rent Prices")
# kable(green.num.table, caption="Number of Observations")
```

According to the median rent table, if a building is Class A, then having a green rating does not appear to change the median rent. This confirms what the lattice plot showed. However, the median rents of non-Class A buildings appear to depend on whether they have green ratings. It would appear that being green matters if the building is not Class A, which is plausible. However, is there skew in the data that may cause the median of non-Class A buildings to mislead us?

### Why Using The Difference in Medians to Calculate Additional Revenue is Misleading

Up until this point, the analysis has ignored the skew in the data and focused on a confounding variable, Class. Let's assume that the difference in median prices between green and non-green buildings (i.e. the "green premium") is still one criteria for determining whether to go "green." Recall that his other criteria is leasing rate - that the guru expects the investment from going "green" will be repaid within 8 years if the leasing rate is also at least 90%. The analysis will now show how subsetting the data by 3 variables - class, green rating, and leasing rate - reveals how fickle the median is. By subsetting, we can see the effects of different variables on the median price, and show that the "green premium" that the guru calculated is misleading.

The guru used the median rent difference between non-green and green buildings, $2.60, as his estimate of the increase in marginal revenue from "going green." He removed buildings with leasing rates below 10% to remove some outliers in the data. 

Let's take a look at how the data is currently distributed. The plot below shows the distribution of leasing rates for both Class A and non-Class A buildings. 

```{r,echo=FALSE}

xyplot(Rent ~ leasing_rate | as.factor(class_a),
       data=greendata,
       group=green_rating,
       strip=strip.custom(factor.levels=c("Not Class A","Class A")),
       ylim=c(0,190),
       main="Green Buildings are Leased: Leasing Rate vs. Rent",
       xlab="Leasing Rate (%)"
       )
```

The guru is justified in removing the buildings with sub-10% leasing rates. There are a large number of non-Class A buildings with sub-10% leasing rates and unless the real estate developer expects to have a sub 10% leasing rate for her building, excluding these buildings makes sense.

Now let's only look at non-Class A buildings and see how leasing rate affects the "green premium". Recall that the premium is the difference in median prices of green and non-green buildings. We have already seen in the previous section that if a building is Class A, being "green" has little impact on the median price. 

```{r,echo=FALSE}
# Line graph
leasing_test = seq(0,99,1)
green.not.a.col = c(1)
not.green.not.a.col = c(1)
diff_median = c(1)
median.diff.lr = expand.grid(leasing_test,green.not.a.col,not.green.not.a.col,diff_median)
colnames(median.diff.lr) = c("Leasing Rate", "Green","Not Green","Median Diff between Green and non-Green")

for (i in 1:99) {
  greendata.i = greendata[greendata$leasing_rate >= i,]
  median.diff.lr[i,2] = median(greendata.i[greendata.i$green_rating == 1 &
                                     greendata.i$class_a == 0,]$Rent)
  median.diff.lr[i,3] = median(greendata.i[greendata.i$green_rating == 0 &                                           greendata.i$class_a == 0,]$Rent)
  median.diff.lr[i,4] = median.diff.lr[i,2] - median.diff.lr[i,3]
}
median.diff.lr = median.diff.lr[-100,]
plot(median.diff.lr[,1],median.diff.lr[,4],
        type="l",
        ylim=c(-5,3),
        main="Green Premium Among Non-Class A Buildings",
        xlab="Leasing Rate",
        ylab="Difference ($)"
)
abline(h=0,lty=2,col='grey')
abline(h=2.60,col='blue',lty=2)
# kable(green.num.table.2, caption="Number of Observations")
```

For non-class A buildings, at every leasing rate level, the "green premium" of non-Class A buildings is lower than than what the guru originally estimated ($2.60, which is colored in blue). What's surprising is that at higher leasing rates, the "green premium" disappears and becomes a "green cost"! _As we have discovered, being Class A certainly matters much more than being green._

Let's now only look at buildings with 90%+ leasing rates. After all, the real estate devloper would want to know the rents of comparable buildings and it can be assumed that she doesn't plan on having a low leasing rate in her building. Recall that among non-Class A buildings, the median of green buildings were higher than that of non-green buildings.

```{r, echo=FALSE}
greendata.10 = greendata[greendata$leasing_rate > 90,]
median.table.2 = as.data.frame(matrix(nrow=3,ncol=3))
rownames(median.table.2) = c("Green", "~Green", "Class Median")
colnames(median.table.2) = c("Class A", "~Class A","Green Rating Median")
median.table.2[1,1] = median(greendata.10[greendata.10$green_rating == 1 &
                                     greendata.10$class_a == 1,]$Rent)
median.table.2[2,1] = median(greendata.10[greendata.10$green_rating == 0 &
                                     greendata.10$class_a == 1,]$Rent)
median.table.2[3,1] = median(greendata.10[greendata.10$class_a == 1,]$Rent)

median.table.2[1,2] = median(greendata.10[greendata.10$green_rating == 1 &
                                     greendata.10$class_a == 0,]$Rent)
median.table.2[2,2] = median(greendata.10[greendata.10$green_rating == 0 &
                                     greendata.10$class_a == 0,]$Rent)
median.table.2[3,2] = median(greendata.10[greendata.10$class_a == 0,]$Rent)

median.table.2[1,3] = median(greendata.10[greendata.10$green_rating == 1,]$Rent)
median.table.2[2,3] = median(greendata.10[greendata.10$green_rating == 0,]$Rent)
median.table.2[3,3] = median(greendata.10$Rent)

green.num.table.2 = xtabs(~green_rating_2 + class_a_2, data=greendata.10)

kable(median.table.2,caption="Median Rent Prices, Leasing Rate > 90%")
```


As we've seen from the previous line graph, if the building is not class A, the median rent of green buildings is actually lower than that of non-green buidings. However, even if the guru had ignored the class of the building and used the difference in median price between green and non-green buildings as his estimate for marginal revenue, the difference in medians among buildings with >90% leasing rates is $1.21, which is more than half of his original estimate of $2.60. Using the median to compute the "green premium" is misleading. It is important to understand the effects of other variables, some of which may inflate the value from going "green."

### Conclusion

This analysis has shown three fallacies in the guru's analysis and analyzed whether calculating a "green premium" is as simple as taking the difference of median rents between green and non-green buildings. It turns out that looking at the median, even if it's statistically significant, and not accounting for other variables is misleading. By introducing other variables such as class and leasing rate and accounting for some of the skew in the data, we have seen that the difference of median rents between green and non-green buildings becomes a less comprehensive metric. There is no substitute for understanding the impact of all possible variables. Although this analysis cannot provide a final recommendation on whether to go "green," the real estate developer should figure out the criteria for being Class A; if being "green" is highly recommended to become Class A, then investing in a "green" building might be worth it.

## III. Bootstrapping

### Overview of the 5 ETF's

```{r bootstrap setup,echo=FALSE}
mystocks.by = c("SPY","TLT","LQD","EEM","VNQ")
myprices.by = yahooSeries(mystocks.by, from="2007-08-01", to='2016-08-01')

# Daily Close of 5 ETFs, 5 years
adj.ret.col = grep('Adj.Close', colnames(myprices.by))

YahooPricesToReturns.by = function(series) {
  mycols = grep('Adj.Close', colnames(series)) # returns columns of adj. close
  closingprice = series[,mycols] # create df of closing prices for stocks
  N = nrow(closingprice) # number of trading days
  percentreturn = as.data.frame(closingprice[2:N,]) / as.data.frame(closingprice[1:(N-1),]) - 1
  mynames = strsplit(colnames(percentreturn), '.', fixed=TRUE) # splits string by removing '.'
  mynames = lapply(mynames, function(x) return(paste0(x[1], ".PctReturn"))) # renames column
  colnames(percentreturn) = mynames
  as.matrix(na.omit(percentreturn))
}
returns_by = YahooPricesToReturns.by(myprices.by)
```

Let's first take a look at how the adjusted closing price of each ETF looks from 8/1/2007 to 8/1/2016. I chose this time frame because it contains periods of both recession and growth.

```{r,echo=FALSE}
adj.ret.col = grep('Adj.Close', colnames(myprices.by))
plot(myprices.by[,adj.ret.col], main="Daily Close, 2007-08-01 to 2016-08-01")
```

At first glance, it would appear that the prices of all 5 ETF's increased over the last 9 years, and spikes or dips in price seem to occur in similar periods. For example, let's take a look at 2008. The SPY, LQD, EEM and VNQ graphs all dip in 2008, with the EEM and VNQ (indexes of emerging markets and real estate, respectively) graphs showing large dips. On the other hand, the TLT (U.S. Treasury 20 Year Index) graph shows a rise in this same time period. The magnitude of dips and spikes, and the correlation of the  price movements among the indexes could be summarized by the _beta_.

### Finding Beta

The plot below shows the returns of all 5 ETF's compared against each other.

```{r,echo=FALSE}
pctret_col = strsplit(colnames(returns_by), '.', fixed=TRUE)
returns_col = lapply(pctret_col, function(x) return(paste0(x[1])))
pairs(YahooPricesToReturns.by(myprices.by),labels=returns_col,
      main="Daily Returns of 5 ETF's")
```

The patterns of these plots confirm some of what we saw from the line plots earlier, and show the relationship among all the funds. In the line plots, when we just looked at 2008, there seemed to be a negative relationship between the indexes of the S&P 500 (SPY) and the U.S. Treasury 20 Year (TLT); while the S&P 500 dipped, the U.S. Treasury 20 year index increased. Their plot shows a negative relationship. Furthermore, the EEM and VNQ plots seemed to mirror the SPY plot, but in a more volatile fashion; both the spikes and dips of these plots were larger. The plots of EEM and VNQ against the SPY plot have a positive linear relationship.

Using SPY as the response, the remaining 4 indexes could be used as the predictors to create a linear regression. The slopes of the resulting regressions are the _betas_. A beta of 1 means that the returns of the index are exactly the same as those of the S&P 500. Assets with betas that are greater than 1 or less than -1 are considered volatile because returns and losses relative to the S&P 500 are magnified. Assets with betas closer to 0 have returns that don't follow the S&P 500 as much, and may be considered safer because returns and losses are reduced. The betas for the 4 indexes relative to the S%P 500 are below.

```{r,echo=FALSE}

beta.table = as.data.frame(matrix(nrow = 4,ncol=1))
rownames(beta.table) = c('TLT','LQD','EEM','VNQ')
colnames(beta.table) = c('Beta')

lm.TLT = glm(returns_by[,2] ~ returns_by[,1])
lm.LQD = glm(returns_by[,3] ~ returns_by[,1])
lm.EEM = glm(returns_by[,4] ~ returns_by[,1])
lm.VNQ = glm(returns_by[,5] ~ returns_by[,1])

beta.table[1,1]=lm.TLT$coefficients[2] # beta is -0.33
beta.table[2,1]=lm.LQD$coefficients[2] # beta is 0.04
beta.table[3,1]=lm.EEM$coefficients[2] # beta is 1.41
beta.table[4,1]=lm.VNQ$coefficients[2] # beta is 1.34

kable(beta.table,digits=3)
```

The betas appear to confirm what we saw in the line graphs. The two indexes with the largest swings, EEM and VNQ, have betas larger than 1. In the case of EEM, a beta of 1.41 means that the EEM's return will be 1.41 times the S&P 500's return. If the S&P 500 index loses $1,000, then the EEM will lose $1,400. If the S&P 500 index increases by $1,000, the EEM will gain $1,400.

On the other hand, TLT has a negative beta, which means that when the S&P 500 index increases, the TLT index slightly decreases. If the S&P 500 index loses $1,000, the TLT gains $330. We have also seen this relationship in the plots of the indexes.


### Using the Betas to Determine Portfolio Allocation
Now that we've seen what the betas mean, we can use them as a guide to allocate stock into our portfolios.

Let's decide how to allocate funds into three types of portfolios: even, risk, and safe.

* Even: 20% of funds will be placed into all 5 index types.
* Risky: To increase returns, a risky portfolio places more weight on funds that have betas greater than 1. However, losses will be magnified.
* Safe: To minimize the impact of losses, a safe portfolio places more weight on funds that have betas closer to 0.

### Simulating Trading Days For Our 3 Portfolios

For the simulation, a trading period of 20 days will be simulated 3000 times. The portfolios are allocated as follows:

* Even: 20% to each asset
* Risky: 40% to both EEM and VNQ; 20% to SPY
* Safe: 40% to both TLT and LQD; 20% to SPY

```{r,echo=FALSE,message=FALSE}

daily_return = resample(returns_by, 1, orig.ids=FALSE)
num_days=20

# Table Setup
portfolio.summary=as.data.frame(matrix(nrow=3,ncol=3))
rownames(portfolio.summary) = c("Even","Risky","Safe")
colnames(portfolio.summary) = c("VAR (5%)","Mean", "SD")

confint.table = as.data.frame((matrix(nrow=3,ncol=2)))
colnames(confint.table) = c("-2.5%","97.5%")
# Portfolio 1: Even split

return.sim.even = foreach(i=1:3000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * totalwealth
  wealthtracker = rep(0, num_days) # Set up a placeholder to track total wealth
  for(today in 1:num_days) {
    return.today = resample(returns_by, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  
  wealthtracker
}

# hist(return.sim.even[,num_days] - 100000,25) # histogram of profit/loss
confint.table[1,1]=confint(return.sim.even[,num_days] - 100000,0.95)[1]
confint.table[1,2]=confint(return.sim.even[,num_days] - 100000,0.95)[2]

portfolio.summary[1,1] = quantile(return.sim.even[,num_days]-100000,0.05)
portfolio.summary[1,2]=mean(return.sim.even[,num_days]-100000)
portfolio.summary[1,3]=sd(return.sim.even[,num_days]-100000)


# Portfolio 2: Risky (explain why it's risky)

return.sim.risky = foreach(i=1:3000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.2, 0.0, 0.0, 0.4, 0.4)
  holdings = weights * totalwealth
  wealthtracker = rep(0, num_days) # Set up a placeholder to track total wealth
  for(today in 1:num_days) {
    return.today = resample(returns_by, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  
  wealthtracker
}

portfolio.summary[2,1] = quantile(return.sim.risky[,num_days]-100000,0.05)
portfolio.summary[2,2]=mean(return.sim.risky[,num_days]-100000)
portfolio.summary[2,3]=sd(return.sim.risky[,num_days]-100000)

confint.table[2,1]=confint(return.sim.risky[,num_days] - 100000,0.95)[1]
confint.table[2,2]=confint(return.sim.risky[,num_days] - 100000,0.95)[2]

# Portfolio 3: Safe (explain why it's safe)

return.sim.safe = foreach(i=1:3000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.2, 0.4, 0.4, 0.0, 0.0)
  holdings = weights * totalwealth
  wealthtracker = rep(0, num_days) # Set up a placeholder to track total wealth
  for(today in 1:num_days) {
    return.today = resample(returns_by, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  
  wealthtracker
}

confint.table[3,1]=(confint(return.sim.safe[,num_days] - 100000,0.95))[1]
confint.table[3,2]=(confint(return.sim.safe[,num_days] - 100000,0.95))[2]                                                                    
portfolio.summary[3,1] = quantile(return.sim.safe[,num_days]-100000,0.05)
portfolio.summary[3,2]=mean(return.sim.safe[,num_days]-100000)
portfolio.summary[3,3]=sd(return.sim.safe[,num_days]-100000)

portfolio.summary2 = cbind(portfolio.summary,confint.table)
kable(portfolio.summary2,digits=3,caption="Summary Statistics - 5000 Simulations of 20 Trading Days")
```

Let's take a look at the results from our simulation.

* VAR (5%) - Value-at-risk at the 5% level
* Mean, SD, and 95% confidence interval show profits (Ending portfolio balance - $100,000 initial investment)

The results show that the riskier the portfolio is, the more volatile it is. The Risky portfolio has a higher value-at-risk (5%) and standard deviation, and has a wider 95% connfidence interval than the Safe and Even portfolios. It also has a higher mean. On the opposite end of the spectrum, the Safe portfolio has the lowest VAR, mean and standard deviation. In addition, it's confidence interval is the least wide due to its lower volatililty.

So how should an investor choose a portfolio allocation? It depends on his or her risk appetite. How much volatility can he or she handle? On average, the Risky portfolio has the highest profit after 20 days of simulated trading, but is extremely volatile. On the other hand, a Safe portfolio has the lowest profit but smaller swings. The Even portfolio is in the middle. If the profits regress to the mean in the long run, then the investor may opt to choose the Risky portfolio.

* If the investor has time on his or her side and doesn't mind huge swings, he or she should opt for a riskier portfolio consisting of assets that have high betas.
* If the investor doesn't like volatility and wants to be protected against drops in the market, he or she should opt for a safe portfolio with assets that have betas close to 0. 
* Investors with preferences in between should opt for an even portfolio.

## IV. Market Segmentation

```{r,echo=FALSE}
tweetdata = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv")
tweetdata = tweetdata[tweetdata$adult != 1 | tweetdata$spam != 1,]
tweetdata = tweetdata[,-1]
n.tweets = apply(tweetdata, MARGIN=1,FUN=sum)
tweetdata.norm = tweetdata/apply(tweetdata, MARGIN=2,FUN=sum)
tweetdata.norm.row.25 = (tweetdata/(apply(tweetdata,MARGIN=1,FUN=sum)))[n.tweets >=25,]
```
The potential to gain insight from social media is enormous in today's world of big data. In its latest quarterly report, Twitter, one of the most popular social media platforms, reported having 313 million average monthly active users, for the three month period ending June 30, 2016. Learning about a company's followers could provide insights about important customer segments. Statistical techniques such as Principal Components Analysis and K Means can help us find insights about NutritionH20's followers and reveal distinct patterns among them.

Before diving into the analysis, working definitions of _interest_ and _market segment_ need to be provided. The "interests" will be the tweet category of a follower. If a follower has many tweets about personal fitness, the analysis will assume that one of the follower's interests is personal fitness. 

A market segment is a group that has a collection of similar interests and that is defined by the common thread of these interests. These interests are not mutually exclusive within a segment and could reside in multiple segments. For example, let's suppose that food is a interest that resides in multiple segments. However, a segment with "food" and "parenting" is distinct from a segment with "food" and "personal fitness"; the former segment may describe parents who cook, and the latter may describe health-conscious individuals.

### Data Overview

Below is a histogram and summary statistics for the number of tweets per user. The distribution of tweets per user is skewed heavily to the right. 

```{r tweets histogram,echo=FALSE}
n.tweets = apply(tweetdata, MARGIN=1,FUN=sum)
hist(n.tweets,breaks = 30,xlab='Total # of Tweets per User',main="Total Tweets per User Distribution")
```

The skew in the data shows a long tail of followers who are extremely active. Among these followers, what are the most popular topics?

```{r,echo=FALSE}
n.tweets.cat = as.data.frame(apply(tweetdata,MARGIN=2,FUN=sum))
n.tweets.cat[,2] = rownames(n.tweets.cat)
rownames(n.tweets.cat) = 1:nrow(n.tweets.cat)
n.tweets.cat = n.tweets.cat[c("V2","apply(tweetdata, MARGIN = 2, FUN = sum)")]
colnames(n.tweets.cat) = c("Category","Number of Tweets")
n.tweets.cat$Category = factor(n.tweets.cat$Category,
                               levels=n.tweets.cat[order(n.tweets.cat[2]),1])

ggplot(mapping=aes(n.tweets.cat[,1],n.tweets.cat[,2])) + geom_point(stat="identity") + coord_flip() + labs(x = 'Number of Tweets', y = 'Tweet Category') + ggtitle("Number of Tweets by Category")
```

Just as the histogram of followers were skewed, the distribution of interests is heavily concentrated in the first 5 interests, and has a long tail beyond that. In fact, "chatter" is by far the most popular tweet type, followed by photo sharing and nutrition. Beyond the top 5 categories is a mixture of all types of tweets from online gaming to parenting.

There are two ways to interpret the skew in followers and tweet categories depending on the type of business problem:

* What are the distinct customer segments that NutritionH20's followers belong to?
* Which of these segments are the most representative of NutritionH20's followers?

For the first point, the data needs to be scaled so that the proportion of tweets of the most active users do not dominate and mask the tweets of less active users. For the second point, after defining all possible segments, the most representative segment(s) of NutritionH20's followers could be identified based on their most popular tweet categories. 

### Defining the Distinct Customer Segments Among NutritionH20's Followers

Using K-Means is particularly useful for creating initial customer segments. Because the distribution of tweets is skewed heavily to the right, the data must be scaled so that each cluster could represent distinct customer segments.

To scale the data, for each user, the number of tweets per category is divided by the user's total number of tweets. As a result, the values of each row are the proportion of a user's tweets that are about a particular category. So even if a user has 200 total tweets, 60% of which are about fitness, her tweets won't overshadow the tweets of a user with 10 tweets and the same proportion of fitness-related tweets.

After scaling the data, the K-Means algorithm could be performed. For each cluster, the top 5 categories ranked by the center coefficients are presented in the table below.

```{r kmeans,echo=FALSE}
set.seed(1)
ZTweet = tweetdata.norm.row.25
kmeans_tweet = cclust(ZTweet,k=5,control=list(initcent="kmeanspp"))
para_summary = (parameters(kmeans_tweet)) # Parameters

km.segments = as.data.frame(matrix(nrow=5,ncol=5)) # Table template
colnames(km.segments) = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5")

k1 = as.data.frame(sort(para_summary[1,],decreasing=TRUE))
k2 = as.data.frame(sort(para_summary[2,],decreasing=TRUE))
k3 = as.data.frame(sort(para_summary[3,],decreasing=TRUE))
k4 = as.data.frame(sort(para_summary[4,],decreasing=TRUE))
k5 = as.data.frame(sort(para_summary[5,],decreasing=TRUE))

km_list = list(k1,k2,k3,k4,k5)
col_num = 1
for (seg in km_list) {
  km.segments[col_num] = head(rownames(seg),5)
  col_num = col_num + 1
}

kable(km.segments, caption="K-Means Clusters")
```

At first glance, the resulting clusters appear to be distinct. Cluster 4 appears to be the "health conscious" group, while Cluster 2 appears to be current events and politics enthusiasts. However, how different are each of these clusters? To measure how distinct each cluster is from each other, the distance between each cluster could be calculated; if two clusters have centers that are far from each other, then those two clusters would be relatively more distinct. However, clusters whose centers are closer together does not necessarily suggest similarity and may instead show the limitations of the K-Means algorithm; certain observations could be more difficult to categorize, but the algorithm has to assign a cluster to them.

```{r cluster diff, echo=FALSE}
set.seed(1)
km.center.table = as.data.frame(matrix(nrow=6,ncol=6))
km.center.table[,1] = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Mean")
colnames(km.center.table) = c("","Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5")
for (row in 1:5) {
  for (col in 2:6) {
    km.center.table[row,col] = sum((para_summary[row,] - para_summary[col-1,])^2)^0.5
  }
}
mean_vector = apply(km.center.table[1:5,2:6],MARGIN=2,FUN=sum)

for(col in 2:6) {
  km.center.table[6,col] = mean_vector[col-1]/4
}

kable(km.center.table, digits=3, caption="K-Means Cluster Centers Distances")
```

The most distinct cluster appears to be cluster 4, whose center has the highest average distance from the other clusters. Based on the category coefficients of its center, Cluster 4 is the "health conscious and active" segment.  So far, it would appear that **the first distinct segment is made up of the health-conscious and active customers.**

Clusters 1-3 have similar average cluster center distances. 

* Cluster 1 is a bit tricky to immediately interpret without help from a subject matter expert. It is difficult to determine whether it represents a cluster or is just a collection of categories that K-Means could not categorize.
* Cluster 2 appears to represent users who are interested in politics and current events. **Its center is quite far from the health conscious group (Cluster 4), so it may be safe to assign Cluster 2 to the "politics and current events" segment.**
* Cluster 3 appears to represent the two most popular tweet categories: chatter and photo sharing. Because of how prevalent these two tweet types are among all of NutritionH20's followers, Cluster 3 does not appear to represent a unique segment.

Finally, Cluster 5 has the smallest average cluster center distance to the remaining 4 clusters. Without knowing anything about NutritionH20, determining whether Cluster 5 is a unique segment or represents shared traits among the remaining 4 clusters is difficult. For example, if NutritionH20's followers are primarily parents from Texas, sports, religion and parenting would certainly be interests that the other 4 clusters share. On the other hand, Cluster 5 could be a distinct segment comprised of religious parents who enjoy sports. **Given how close its center is to the other clusters, Cluster 5 could be the segment that is comprised of the "God and Football" parents, but determining whether this segment overlaps with other segments is tricky.**

K-Means has defined 3 distinct segments but it is unclear whether there is overlap among these segments. Some questions that remain are:

* Does the health conscious group contain both young and older people? 
* What else do health conscious users tweet about?

The results from Principal Component Analysis begin to answer some of these questions.

```{r PCA, echo=FALSE}
set.seed(1)
pc4 = prcomp(tweetdata.norm.row.25,scale=FALSE)

loadings4 = pc4$rotation
scores4 = pc4$x

# pve = 100*pc4$sdev^2/sum(pc4$sdev^2)
# plot(pve,xlab='Principal Component',main="Scree Plot",col='blue') # scree plot

o4.1 = order(loadings4[,1]) # Differences in age and lifestyle
p.1.1 = colnames(tweetdata.norm.row.25)[head(o4.1,8)] # religion, parenting, food; non gender
p.1.2 = colnames(tweetdata.norm.row.25)[tail(o4.1,8)] # college,fitness,fashion,shopping, female?

o4.2 = order(loadings4[,2]) # Interests
p.2.1 = colnames(tweetdata.norm.row.25)[head(o4.2,8)] # politics, cars, news, college, tv
p.2.2 = colnames(tweetdata.norm.row.25)[tail(o4.2,8)] 

# Tables
pca.comp1 = as.data.frame(matrix(nrow=8,ncol=2)) 
colnames(pca.comp1) = c("Top 8", "Bottom 8")

pca.comp1[,1] = p.1.1
pca.comp1[,2] = p.1.2

pca.comp2 = as.data.frame(matrix(nrow=8,ncol=2)) 
colnames(pca.comp2) = c("Top 8", "Bottom 8")

pca.comp2[,1] = p.2.1
pca.comp2[,2] = p.2.2

kable(pca.comp1,caption="Component 1")
kable(pca.comp2,caption="Component 2")



```

For each component, the "top 8" and "bottom 8" represent categories that have the highest and lowest scores, and show which categories are the most different. Just as the K-Means clusters showed, the PCA analysis reveal a split in health/nutrition tweets and politics/current events tweets. In both components 1 and 2, the "Top 8" categories deal with health/nutrition and the "Bottom 8" categories deal with politics/current events, providing additional evidence that these 2 segments are different.

Both components also reveal additional interests in both the health conscious and politics enthusiast segments. These additional interests provide additional data on how nuanced each segment is. For example, in the first component, health conscious users are also interested in fashion and an eco lifestyle. On the other hand, politics enthusiasts are also interested in cars and travel. The age of the users in health conscious segment appear to be on the younger side.

### Conclusion

Based on the K-Means clusters and Principal Components Analysis, the following segments have been identified:

* _Health conscious and active_: Based on how popular health/nutrition tweets are, this segment may be the most representative of NutritionH20's followers. Additional interests within this segment include fashion, eco lifestyle, and the outdoors, suggesting that the users in this segment are younger.
* _Politics and current events enthusiasts_: Additional interests include sports, automobiles, and travel.
* _Religious parents_: Because the categories associated with religion and parenting were mostly absent from the first 2 principal components, assigning "religious parents" as the third segment makes sense, although it is difficult to determine how relevant this segment is to NutritionH20.

In general, K-Means helped identify all possible segments and PCA helped show additional nuances within these segments. These segments were identified based solely on Twitter data. Without knowing anything about the type of business NutritionH20 is engaged in or its business goals, providing additional analysis based on the segments is difficult. However, NutritionH20 now has customer data about its followers that can be used to initiate further customer analyses.





